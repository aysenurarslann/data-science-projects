# -*- coding: utf-8 -*-
"""kairu-nlp1-9hafta-odev.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SlICrOut5Im8BUt4Q2J-jeK0sOPAY82z
"""

!pip install nltk spacy matplotlib wordcloud
!python -m spacy download en_core_web_sm

import nltk
import spacy
import re
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer
from nltk import pos_tag

# NLTK veri indirme
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')

# spaCy modeli yükleme
nlp = spacy.load("en_core_web_sm")

# Metin
text = """
On January 3rd, 2023, Dr. Emily Watson, a senior data scientist at GreenAI Inc., gave a keynote speech at the International Conference on Artificial Intelligence in Paris, France. During her talk, she emphasized the importance of ethical AI and data privacy, citing recent cases of misuse in various industries.

She mentioned that over 3.2 million users were affected by a data breach last year, resulting in damages estimated at $12.5 million. Furthermore, she highlighted the role of open-source libraries, such as spaCy and NLTK, in democratizing access to natural language processing tools. According to her, students and researchers can now build high-quality NLP models without needing large financial resources.

"AI is not just about machines," she said, "it’s about how we interact with technology in a human-centered way." After the session, attendees from universities like Stanford, MIT, and Oxford approached her to discuss future collaboration opportunities.

At 5:45 PM, she posted a summary of her speech on Twitter, receiving over 8,000 likes and 1,200 retweets within a few hours. Her tweet included hashtags like #AIethics, #DataPrivacy, and #NLPtools.

The event concluded with a panel discussion moderated by Mr. John Lee, a journalist from TechWorld Weekly, who asked, “How can governments regulate AI without stifling innovation?”
"""

# 1. Tokenization (Cümlelere ve kelimelere ayırma)
sentences = sent_tokenize(text)
words = word_tokenize(text)

print("1. Tokenization")
print("Cümleler:", sentences[:2])  # İlk 2 cümleyi yazdır
print("Kelimeler:", words[:10])    # İlk 10 kelimeyi yazdır

# 2. Lowercasing (Tüm kelimeler küçük harf)
words_lower = [word.lower() for word in words]

print("\n2. Lowercasing")
print(words_lower[:10])

# 3. Removing Punctuation (Noktalama işaretlerini kaldırma)
words_no_punct = [word for word in words_lower if word.isalnum()]

print("\n3. Removing Punctuation")
print(words_no_punct[:10])

# 4. Removing Stopwords (Anlamsız kelimeleri kaldırma)
stop_words = set(stopwords.words('english'))
words_no_stop = [word for word in words_no_punct if word not in stop_words]

print("\n4. Removing Stopwords")
print(words_no_stop[:10])

# 5. Removing Numbers (Sayıları temizleme)
words_no_num = [word for word in words_no_stop if not re.search(r'\d', word)]

print("\n5. Removing Numbers")
print(words_no_num[:10])

# 6. Stemming (Kelime köklerine indirgeme)
stemmer = PorterStemmer()
words_stemmed = [stemmer.stem(word) for word in words_no_num]

print("\n6. Stemming")
print(words_stemmed[:10])

# 7. Named Entity Recognition (NER) - Kişi, yer, kurum vs.
doc = nlp(text)
entities = [(ent.text, ent.label_) for ent in doc.ents]

print("\n7. Named Entity Recognition")
for ent in entities:
    print(ent)

# 8. Part-of-Speech (POS) Tagging
pos_tags = pos_tag(words_no_stop)

print("\n8. Part-of-Speech Tagging")
print(pos_tags[:10])

# 9. Word Frequency Count (En sık geçen kelimeler)
word_freq = Counter(words_no_stop)
most_common = word_freq.most_common(10)

print("\n9. Word Frequency Count")
print(most_common)

# 10. Text Visualization - Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(words_no_stop))

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud of Processed Text")
plt.show()

# Bar Chart for Word Frequencies
words, counts = zip(*most_common)
plt.figure(figsize=(10, 5))
plt.bar(words, counts)
plt.title("Top 10 Most Common Words")
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.show()

